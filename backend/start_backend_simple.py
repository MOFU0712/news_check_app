#!/usr/bin/env python3
"""
ç°¡æ˜“ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆèªè¨¼ã®ã¿ï¼‰
"""
import uvicorn
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional

# ç°¡æ˜“ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="ITãƒ‹ãƒ¥ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç°¡æ˜“ç‰ˆï¼‰",
    description="èªè¨¼æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆç”¨",
    version="1.0.0"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ç°¡æ˜“ã‚¹ã‚­ãƒ¼ãƒ
class LoginRequest(BaseModel):
    email: str
    password: str

class LoginResponse(BaseModel):
    access_token: str
    token_type: str = "bearer"

class UserResponse(BaseModel):
    id: str
    email: str
    role: str
    is_active: bool = True

# ç°¡æ˜“èªè¨¼ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
@app.post("/api/auth/login", response_model=LoginResponse)
async def login(request: LoginRequest):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ­ã‚°ã‚¤ãƒ³"""
    # å›ºå®šã®ç®¡ç†è€…ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
    if request.email == "admin@example.com" and request.password == "admin123":
        return LoginResponse(
            access_token="test-jwt-token-admin",
            token_type="bearer"
        )
    # å›ºå®šã®ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼
    elif request.email == "user@example.com" and request.password == "user123":
        return LoginResponse(
            access_token="test-jwt-token-user", 
            token_type="bearer"
        )
    else:
        raise HTTPException(status_code=401, detail="Incorrect email or password")

@app.get("/api/auth/me", response_model=UserResponse)
async def get_current_user():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±"""
    return UserResponse(
        id="test-user-id",
        email="admin@example.com",
        role="admin"
    )

@app.get("/api/auth/validate-token")
async def validate_token():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼"""
    return {"valid": True, "user_id": "test-user-id"}

# URLãƒ‘ãƒ¼ã‚µãƒ¼ã¨Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’è¿½åŠ 
from app.utils.url_parser import URLParser
from app.utils.web_scraper import WebScraper

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢é€£ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/api/scrape/parse-urls")
async def parse_urls(request: dict):
    """URLãƒ‘ãƒ¼ã‚¹ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ API"""
    urls_text = request.get("urls_text", "")
    
    if not urls_text.strip():
        return {
            "valid_urls": [],
            "invalid_urls": [],
            "duplicate_urls": [],
            "summary": {"valid_count": 0, "invalid_count": 0, "duplicate_count": 0, "total_lines": 0},
            "estimated_time": "0ç§’"
        }
    
    # URLãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
    parse_result = URLParser.parse_urls_from_text(urls_text)
    
    # æ—¢å­˜è¨˜äº‹ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰
    existing_urls = {
        "https://example.com/existing1",
        "https://blog.example.com/existing2"
    }
    
    new_urls, duplicate_urls = URLParser.check_duplicates_with_existing(
        parse_result.valid_urls, existing_urls
    )
    
    return {
        "valid_urls": new_urls,
        "invalid_urls": [{"url": item[0], "reason": item[1]} for item in parse_result.invalid_urls],
        "duplicate_urls": duplicate_urls,
        "summary": {
            "valid_count": len(new_urls),
            "invalid_count": len(parse_result.invalid_urls),
            "duplicate_count": len(duplicate_urls),
            "total_lines": parse_result.total_input_lines
        },
        "estimated_time": URLParser.estimate_processing_time(len(new_urls))
    }

@app.post("/api/scrape/preview")
async def preview_url(request: dict):
    """URLãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ APIï¼ˆå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œï¼‰"""
    url = request.get("url", "")
    
    # URLæ­£è¦åŒ–
    normalized_url = URLParser._normalize_url(url)
    if not normalized_url:
        return {"error": "Invalid URL format"}
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
    existing_urls = {"https://example.com/existing1", "https://blog.example.com/existing2"}
    is_duplicate = normalized_url in existing_urls
    
    try:
        # å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        async with WebScraper(timeout=15) as scraper:
            scraped_content = await scraper.scrape_url(normalized_url)
            
            if scraped_content.error:
                return {
                    "url": normalized_url,
                    "title": None,
                    "description": None,
                    "site_name": None,
                    "is_duplicate": is_duplicate,
                    "estimated_tags": [],
                    "error": scraped_content.error
                }
            
            return {
                "url": normalized_url,
                "title": scraped_content.title,
                "description": scraped_content.description,
                "site_name": scraped_content.site_name,
                "is_duplicate": is_duplicate,
                "estimated_tags": scraped_content.auto_tags,
                "error": None
            }
            
    except Exception as e:
        return {
            "url": normalized_url,
            "title": None,
            "description": None,
            "site_name": None,
            "is_duplicate": is_duplicate,
            "estimated_tags": [],
            "error": f"Preview failed: {str(e)}"
        }

# è¨˜äº‹é–¢é€£ã®ãƒ€ãƒŸãƒ¼ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/api/articles")
async def list_articles():
    """ãƒ€ãƒŸãƒ¼è¨˜äº‹ä¸€è¦§"""
    return {
        "articles": [
            {
                "id": "article-1",
                "title": "React 18ã®æ–°æ©Ÿèƒ½ï¼šConcurrent Featuresã®ä½¿ã„æ–¹",
                "url": "https://example.com/react18-concurrent",
                "content": "React 18ã§ã¯ã€Concurrent Featuresã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„æ©Ÿèƒ½ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸ...",
                "source": "Reactå…¬å¼ãƒ–ãƒ­ã‚°",
                "tags": ["React", "JavaScript", "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰"],
                "summary": "React 18ã§å°å…¥ã•ã‚ŒãŸConcurrent Featuresã®æ¦‚è¦ã¨ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦è§£èª¬ã€‚",
                "scraped_date": "2024-01-15T10:30:00Z",
                "created_at": "2024-01-15T10:30:00Z",
                "updated_at": "2024-01-15T10:30:00Z",
                "is_favorite": False
            },
            {
                "id": "article-2", 
                "title": "FastAPIã§ãƒ¢ãƒ€ãƒ³ãªWeb APIé–‹ç™º",
                "url": "https://example.com/fastapi-modern-api",
                "content": "FastAPIã¯ã€Pythonã§ãƒ¢ãƒ€ãƒ³ãªWeb APIã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®é«˜æ€§èƒ½ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™...",
                "source": "Python Weekly",
                "tags": ["Python", "FastAPI", "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰"],
                "summary": "FastAPIã‚’ä½¿ã£ãŸãƒ¢ãƒ€ãƒ³ãªWeb APIé–‹ç™ºã®æ‰‹æ³•ã‚’ç´¹ä»‹ã€‚",
                "scraped_date": "2024-01-14T14:20:00Z",
                "created_at": "2024-01-14T14:20:00Z", 
                "updated_at": "2024-01-14T14:20:00Z",
                "is_favorite": True
            }
        ],
        "total": 2,
        "page": 1,
        "limit": 20,
        "has_next": False,
        "has_prev": False
    }

@app.get("/api/articles/stats/overview")
async def get_stats():
    """ãƒ€ãƒŸãƒ¼çµ±è¨ˆæƒ…å ±"""
    return {
        "total_articles": 2,
        "monthly_articles": 2,
        "popular_tags": [["React", 1], ["Python", 1], ["JavaScript", 1]],
        "source_stats": [["Reactå…¬å¼ãƒ–ãƒ­ã‚°", 1], ["Python Weekly", 1]]
    }

@app.get("/")
async def root():
    return {"message": "ITãƒ‹ãƒ¥ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ†ã‚¹ãƒˆç‰ˆï¼‰ãŒèµ·å‹•ä¸­"}

@app.get("/health")
async def health():
    return {"status": "healthy"}

if __name__ == "__main__":
    print("ğŸš€ ç°¡æ˜“ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™...")
    print("ğŸ“ URL: http://localhost:8000")
    print("ğŸ“– APIä»•æ§˜æ›¸: http://localhost:8000/docs")
    print("")
    print("ğŸ” ãƒ†ã‚¹ãƒˆç”¨ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±:")
    print("  ç®¡ç†è€…: admin@example.com / admin123")
    print("  ä¸€èˆ¬: user@example.com / user123")
    print("")
    
    uvicorn.run(
        "start_backend_simple:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )